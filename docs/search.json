[
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "Reddit Data Structure\n\n\n\n Back to top"
  },
  {
    "objectID": "feedback.html",
    "href": "feedback.html",
    "title": "Feedback Discussion",
    "section": "",
    "text": "Throughout our work on this project, we received feedback from our instructors. Professor Marck was especially helpful in providing as with feedback as our faculty mentor for this work. The feedback we received was invaluable in helping us improve our project and ensure that we were constructing the most helpful analyses for our project aims.\n\n\n\n\n\n\nImage courtesy of Flaticon",
    "crumbs": [
      "Home",
      "Feedback Discussion"
    ]
  },
  {
    "objectID": "feedback.html#project-plans",
    "href": "feedback.html#project-plans",
    "title": "Feedback Discussion",
    "section": "Project plans",
    "text": "Project plans\n\nFeedback\n\nExplore the entirety of reddit outside of those subreddits as well for anything relevant.\nIncorporate recent data as your external data\n\n\n\nImplementation\n\nWe expanded our initial list of four subreddits to include more subreddits that were relevant to our project aims. While our analytical set only had six subreddits, our initial EDA and discovery used nine subreddits which we then narrowed down based on relevance and content size.\nWe incorporated recent data by pulling data from Google trends and incorporating it into our analysis. This analysis is used in the NLP section of our project.",
    "crumbs": [
      "Home",
      "Feedback Discussion"
    ]
  },
  {
    "objectID": "feedback.html#eda",
    "href": "feedback.html#eda",
    "title": "Feedback Discussion",
    "section": "EDA",
    "text": "EDA\n\nFeedback\n\nShow distributions rather than just central tendencies in plots\nNormalize data to compare across subreddits\nUse explanatory titles and labels\nAnalyze threads, not just comments or submissions\n\n\n\nImplementation\n\nWe updated our plots to use fewer bar charts and instead incorporated more density plots, line charts, and histograms to show distributions. When measuring central tendencies, we included averages and medians to provide a more comprehensive view of the data.\nWe normalized score when creating density plots to allow for clear comparison across subreddits. This normalization was especially helpful for understanding our largest subreddit, r/politics, in context.\nWe updated our titles and labels significantly for more clarity. We also added subtitles to all plots and tables for additional context.\nWe created an analysis of threads in the Comment Controversiality section of EDA. This analysis allowed us to understand trends at the thread level in different subreddits.",
    "crumbs": [
      "Home",
      "Feedback Discussion"
    ]
  },
  {
    "objectID": "feedback.html#nlp",
    "href": "feedback.html#nlp",
    "title": "Feedback Discussion",
    "section": "NLP",
    "text": "NLP\n\nFeedback\n\nUse a pre-trained model from Spark to create new features\nConsider using a Name Entity Recognition (NER) model to identify entities in the larger body of subreddit text\n\n\n\nImplementation\n\nWe implemented a pre-trained model from Spark to do sentiment analysis on our data. This analysis is used in the NLP section of our project.\nWe considered an NER model and even experimented with running it. However, we ultimately decided that the insights from the model were not helping us move towards our project aims. In future work, we may revisit this model to see if it can provide additional insights, but NER did not seem to be giving us helpful information for selecting which data to use in our analysis. We did decide to add a topic modeling section to our NLP analysis using Latent Dirichlet Allocation (LDA) to identify topics in our data.",
    "crumbs": [
      "Home",
      "Feedback Discussion"
    ]
  },
  {
    "objectID": "feedback.html#ml",
    "href": "feedback.html#ml",
    "title": "Feedback Discussion",
    "section": "ML",
    "text": "ML\n\nFeedback\n\nConsider the usefulness of subreddit prediction. What is the use case for this?\n\n\n\nImplementation\n\nTo address this, we first added more context to our discussion of the subreddit prediction in order to explain how it might be a useful analysis. The analysis was particularly useful in giving us insight into the separability of conversations happening in different subreddits. Second, we also added a new machine learning analysis to our project that predicts the number of comments for a post. This analysis is more directly useful for understanding the engagement a post is likely to receive and could be used by a social media manager for a political campaign or advocacy group to understand how to best engage with their audience.",
    "crumbs": [
      "Home",
      "Feedback Discussion"
    ]
  },
  {
    "objectID": "feedback.html#website",
    "href": "feedback.html#website",
    "title": "Feedback Discussion",
    "section": "Website",
    "text": "Website\n\nFeedback\n\nNone\n\n\n\nImplementation\n\nWe did not receive any feedback on our website. However, we have worked hard to ensure that our website is clear, easy to navigate, visually appealing, and informative.",
    "crumbs": [
      "Home",
      "Feedback Discussion"
    ]
  },
  {
    "objectID": "ml.html",
    "href": "ml.html",
    "title": "Machine Learning",
    "section": "",
    "text": "In this section, we used machine learning to predict the subreddit, popularity, political leaning, and number of comments of submissions in our Reddit dataset. Our goal was to understand how the contents of a submission to Reddit in the political sphere contributes or delineates to its location, popularity (score and comment count), and political leaning. In Table 1, we summarize the results of our machine learning models.\n\n\n\n\nTable 1: Results for the best models of each prediction task.\n\n\n\n\n\n\n\n\n\n\nMachine Learning Results for Predicting Reddit Submission Features\n\n\nSummary of the results of the best machine learning models for predicting subreddit, score, political leaning, and number of comments.\n\n\n\nBest Model\nPerformance Metrics\n\n\nAccuracy\nPrecision\nRecall\nF1 Score\n\n\n\n\nSubreddit Prediction\nDecision Tree\n56.0%\n57.0%\n56.0%\n55.7%\n\n\nPopularity Prediction\nDecision Tree\n73.7%\n60.0%\n54.7%\n56.6%\n\n\nPolitical Leaning Prediction\nRandom Forest\n83.2%\n58.3%\n54.0%\n55.7%\n\n\nComment Count Prediction\nRandom Forest\n76.8%\n64.8%\n64.2%\n64.4%\n\n\n\n\n\n\n\n\n\n\n\nOverall, we see that the subreddit classification task helps us uncover some separability between political subreddits. In particular, we identify separability for r/politics and r/Libertarian, while other subreddits in our analytical set are harder to distinguish. We also identify that the word “Biden” is a key feature for helping distinguish between subreddits, giving us more information about the types of conversations happening on different political subreddits. Through our popularity prediction task using score, we find that modeling is reasonably effective at identifying which posts are likely to become popular and achieve high scores. Our political leaning prediction model shows us that conservative and liberal posts are easy to distinguish, but libertarian posts are more challenging to identify. Finally, the comment count prediction task shows us that the score and author of a post are the most important features in predicting the number of comments a post will receive, and it sets us up for future work to predict the number of comments without using the score feature.",
    "crumbs": [
      "Home",
      "ML"
    ]
  },
  {
    "objectID": "ml.html#subreddit-prediction",
    "href": "ml.html#subreddit-prediction",
    "title": "Machine Learning",
    "section": "Subreddit Prediction from Submission Text",
    "text": "Subreddit Prediction from Submission Text\nOne of our goals was to predict the subreddit of a submission based on its text and other keys features including number of comments, score, and sentiment. Through subreddit prediction, we were able to determine which features were most important in categorizing a submission, and we could see how separable posts were across different subreddits.\nTo predict the subreddit, we first converted the text to a bag of words representation which was then transformed into a tf-idf matrix. We then combined this matrix with the other features and hypertuned four different classifiers: logistic regression, random forest, decision tree, and naive bayes. Due to the inequity in the number of submissions from each subreddit, we balanced the classes by over/undersampling each class to the median size of the classes. Each model was then trained and hypertuned using k-fold cross validation to identify the best performing hyperparameters for the data.\nOverall, the decision tree classifier performed best with an accuracy of 56%. While this is not a high accuracy, it is still better than the most common feature accuracy 17%, meaning we have learned to explain some of the variance in the data. From Figure 1, we see that r/politics and r/Libertarian are the most easily distinguishable subreddits using our model, while r/democrats and r/Conservative are the least distinguishable. Also, we see that r/politics seems to be somewhat representative of a wide range of political ideas based on the misclassifications of other subreddits as r/politics.\n\n\n\n\n\n\nFigure 1: Confusion Matrix for Decision Tree Classifier for Subreddit Prediction\n\n\n\nThrough this modeling exercise, we were able to identify the key features and words that helped distinguish posts from different subreddits in the political sphere. As seen in Figure 2, the most important features in predicting the subreddit of a submission were the author, the number of comments, and the presence of the word “Biden”. Interestingly, the presence of the word “Trump” was not as important in predicting the subreddit of a submission.\n\n\n\n\n\n\nFigure 2: Feature Importances for Decision Tree Classifier for Subreddit Prediction\n\n\n\n\n\n\n\n\n\nThe code used for this section is available here.",
    "crumbs": [
      "Home",
      "ML"
    ]
  },
  {
    "objectID": "ml.html#popularity-prediction",
    "href": "ml.html#popularity-prediction",
    "title": "Machine Learning",
    "section": "Predicting Popularity of Posts",
    "text": "Predicting Popularity of Posts\nTo help us understand the influence of posts in the political sphere of Reddit, we decided to build a predictive model to determine the popularity of a post. Popularity is defined as the score, or number of upvotes minus downvotes, that a post receives. Generally, posts with higher scores have more visibility and interaction from users, indicating their influence in the political sphere.\nOur team started by training multiple regression models to predict the score of a post based on its text and other key features. However, after getting \\(R^2\\) values of &lt;0.4, meaning only 40% of the variance in values was explained by the model, we decided to change our approach. We bucketed the scores of posts into logarithmically spaced bins, ranging from a score of 0 to 1,000 and above. The text was then converted TF-IDF using a hashing trick, and we trained four models: logistic regression, random forest, decision tree, and naive bayes. These models were all hypertuned using k-fold cross validation to identify the best performing hyperparameters for the data.\nOnce again, the decision tree model performed best with an accuracy of 74%. This means that based on a post’s texts and a few other attributes, we can predict it’s popularity about 3 out of 4 times. From the ROC AUC curve in Figure 3, we see that the model is able to predict the popularity of posts with a high degree of accuracy across all classes. The model struggles the most with posts with a small score of 2-9 and excels at posts with a score of 1000 and above.\n\n\n\n\n\n\nFigure 3: ROC Curve for Decision Tree Classifier for Popularity Prediction\n\n\n\n\n\n\n\n\n\nThe code used for this section is available here.",
    "crumbs": [
      "Home",
      "ML"
    ]
  },
  {
    "objectID": "ml.html#political-leanings",
    "href": "ml.html#political-leanings",
    "title": "Machine Learning",
    "section": "Predicting Political Leaning of Comments and Posts",
    "text": "Predicting Political Leaning of Comments and Posts\nAnother one of our ML goals was to see if we could predict the political party that a submission best fit into. To do this, we created a variable ‘party’ using submissions subreddit. We classified submissions from r/Conservative and r/Republican as being conservative, posts from r/democrats and r/Liberal as being liberal, and posts from r/Libertarian as being libertarian. We then created a feature vector similar to that used previously with information on the content and sentiment of the submission and the submissions text. We then trained a random forest classifier on the data. We were able to achieve an accuracy of ~83%. However, this is less impressive considering the most common feature accuracy is ~77%. Figure 4 shows that the model generally performs well but tends to predict that most submissions are conservative. This is likely due to the fact that the conservative class is the largest class in the data.\n\n\n\n\n\n\nFigure 4: Confusion Matrix for Political Leaning Prediction\n\n\n\nThe ROC curve in Figure 5 shows that the model performs well for predicting conservative and liberal submissions but struggles with libertarian submissions. This may be because often libertarian submissions are more similar to conservative submissions.\n\n\n\n\n\n\nFigure 5: ROC Curve for Political Leaning Prediction\n\n\n\nFigure 6 shows the feature importances of the top features in the random forest classifier. We can see from the graph that the number of comments and the score were by far the highest predictors. This was a surprise, given that in the subreddit prediction results we found that the author had the highest feature importance. Another interesting result is that the sentiment score had very little importance on the final result. This may indicate that the sentiment between liberal and conservative subreddits is similar.\n\n\n\n\n\n\nFigure 6: Feature Importances for Random Forest Classifier for Political Leaning Prediction\n\n\n\n\n\n\n\n\n\nThe code used for this section is available here.",
    "crumbs": [
      "Home",
      "ML"
    ]
  },
  {
    "objectID": "ml.html#num-comments",
    "href": "ml.html#num-comments",
    "title": "Machine Learning",
    "section": "Predicting Number of Comments",
    "text": "Predicting Number of Comments\nOur last ML goal was to predict the number of comments for any given submission. The number of comments can be seen as a proxy to the controversiality of the post, as more people will weigh in on more controversial topics. We divided our data into three categories containing posts with a large number of comments, posts with a moderate number of comments, and posts with very few or no comments. Like our previous ML experiments, we used a feature vector based on the features in our ML dataframe incuding features like author, is_biden (mentions Biden), is_trump (mentions Trump), score, and the sentiment analysis of the main text among others. We found a random forest model to perform reasonably well, yielding an accuracy of 76%. Given we have three categories this is decent but not extraordinary. As seen in Figure 7, the accuracy for a post having a small number of comments was 84%, while the accuracy for a post having many comments was 78%. The worst categorical accuracy was for a post containing a moderate number of comments, at 27% accuracy.\n\n\n\n\n\n\nFigure 7: Confusion Matrix for Number of Comments Prediction\n\n\n\nThe ROC curve in Figure 8 similarly shows that the model performs well for predicting posts with a small or large number of comments but struggles with posts with a moderate number of comments.\n\n\n\n\n\n\nFigure 8: ROC Curve for Number of Comments Prediction\n\n\n\nWe were also able to calculate the feature importances of our random forest model. In Figure 9, we can see that the most important features by far are score and author, with score being the most important. Score has a feature importance of 0.603, while author has a feature importance of 0.369. Sentiment is the third most important feature, but it is far from the top two, with a feature importance of 0.007. All of the remaining features are below 0.004 feature importance.\n\n\n\n\n\n\nFigure 9: Feature Importances for Random Forest Classifier for Number of Comments Prediction\n\n\n\nSince score is highly correlated with the number of comments, it is not surprising that it is the most important feature in predicting the number of comments. In future analyses, we will consider removing score from the feature vector to see how well we can predict the number of comments without this feature.\n\n\n\n\n\n\nThe code used for this section is available here.",
    "crumbs": [
      "Home",
      "ML"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "Politics and governmental rule have always been a source of discussion and debate in societies throughout history. Today, debate and conversation plays a key part in democracy. In the United States, voters are encouraged to engage in conversations with like-minded peers and dissidents to understand others’ opinions, make informed decisions, and allow all voices to be heard. In the age of the internet, there are more opportunities than ever to engage in discussions regarding politics. Reddit is a popular social media platform full of communities engaging in regular political discussions. Users participate in topical forums called “subreddits” to voice their opinions, share information, and engage with others. Political discussions online point to real-world events and their repercussions in the thoughts and opinions of everyday Americans.\n\n\n\n\n\n\nImage courtesy of Flaticon",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#detailed-project-business-goals",
    "href": "index.html#detailed-project-business-goals",
    "title": "Introduction",
    "section": "Detailed Project Business Goals",
    "text": "Detailed Project Business Goals\n\nEDA\n\nIdea 1: Subreddit Political Engagement\nBusiness goal: Identify which subreddits frequently discuss the President and U.S. politics to understand the spread of political discourse across Reddit.\nTechnical proposal: Use PySpark to filter and aggregate posts mentioning the President, U.S. politics, and elections, grouping by subreddit. Analyze and visualize the distribution of posts across subreddits to see whether discussions are concentrated in subreddits dedicated to politics (such as r/politics) or are prevalent in general-interest subreddits. Normalize the data to account for differences in subreddit sizes.\n\n\nIdea 2: Controversiality vs. Comment Count\nBusiness goal: Analyze the relationship between the controversiality of posts and comment count to understand user engagement during politically charged discussions.\nTechnical proposal: Use the controversiality metric encoded in the data to identify controversial posts. Compare the average comment count and score of controversial posts to non-controversial posts. Compare the distribution of controversial posts across different subreddits to identify which subreddits tend to have more controversial discussions.\n\n\nIdea 3: Distribution of Political Contributions by Top Authors\nBusiness goal: Examine the distribution of political contributions mentioning Biden, Harris, or Trump in each different subreddit by the top authors. Are there a few people making up the majority of the posts?\nTechnical proposal: We will detect the political contributions by finding the Keywords Trump, Biden, and Harris. We then determine the top posters of this political content for each subreddit, and calculate the percent of the total political posts that are authored by these contributors.\n\n\n\nNLP\n\nIdea 4: Comparison Between Reddit Sentiment and Presidential Job Approval Rates\nBusiness Goal: Analyze how sentiment trends on political subreddits focused on Trump and Biden correlate with broader public opinion as measured by Presidential Job Approval Rates. This analysis aims to understand whether shifts in online sentiment, as expressed in subreddit discussions, reflect changes in real-world political perceptions or highlight discrepancies between online and offline public opinion over time.\nTechnical Proposal: Apply sentiment analysis to submissions from the selected political subreddits. Aggregate positive sentiments for each month on Trump and Biden related submissions. Compare these sentiment trends with monthly Presidential Job Approval Rates to see how subreddits sentiments align with the public political views. Conduct comparative statistical test to see if there are any correlations.\n\n\nIdea 5: Lexical Trends Analysis of Key Terms in Trump and Biden Related Reddit Submissions\nBusiness Goal: Explore the textual content of posts in political subreddits to identify commonly used words and topic-specific terms in discussions centered on Trump and Biden. This analysis seeks to reveal the distinctive vocabulary used across communities and highlight the themes and issues driving political discourse in different subreddits.\nTechnical Proposal: Implement Count Vectorization to calculate the raw frequency of words across subreddit submissions, providing insights into the most commonly used terms. Additionally, use Latent Dirichlet Allocation (LDA) to uncover underlying topic structures and associate key terms with thematic discussions. By combining these approaches, identify how different subreddits approach Trump- or Biden-related discussions and which terms are unique or prominent in these conversations. Compare findings across subreddits to highlight contrasts in discourse.\n\n\nIdea 6: Impact of Dominant Terms on Shaping Political Discussions on Reddit\nBusiness Goal: Identify the most impactful terms in shaping political discussions on Reddit by quantifying their importance across subreddits. This analysis seeks to uncover how specific terms drive or influence political conversations in various communities.\nTechnical Proposal: Apply Term Frequency-Inverse Document Frequency (TF-IDF) analysis to identify the most significant terms from a subset of key words derived from Count Vectorizer and LDA outputs. This technique will calculate the relative importance of words within specific subreddits, factoring in their prevalence across the entire Reddit dataset. Combine TF-IDF results with sentiment analysis and topic modeling to contextualize how these terms contribute to shaping subreddit-specific political narratives.\n\n\nIdea 7: Identifying the Most Extreme Subreddits in Political Discourse\nBusiness Goal: Determine which political subreddits foster the most extreme or distinct discussions by analyzing sentiment, frequently used terms, and dominant topics. The goal is to identify the communities that generate the most distinct and possibly divisive political narratives.\nTechnical Proposal: Use the results from sentiment analysis, Count Vectorization, and LDA to identify various trends in sentiments, frequently used words, and topic-specific terms within the submissions. Compare these results across subreddits by quantifying differences in sentiment distributions, lexical diversity, and topic extremity.\n\n\n\nML\n\nIdea 8: Subreddit Prediction from Submission Text\nBusiness goal: Predict the most likely subreddit for a given post based on its text content to improve content recommendations and classification.\nTechnical proposal: Train a multi-class classifier on text data with labels as subreddit names. Use embeddings or TF-IDF vectors as features. Train across political subreddits and evaluate the model’s performance on unseen data. Analyze which words or phrases contribute most to subreddit prediction, and evaluate model’s performance with a confusion matrix and classification metrics.\n\n\nIdea 9: Predicting Popularity of Posts\nBusiness goal: Identify which factors contribute to a post’s popularity (score) to better understand what drives user engagement in political discussions.\nTechnical proposal: Use post score as the target variable. Use post text, post title, comment count, length, and subreddit name as features. Make sure to normalize all values for subreddit size, since posts in larger subreddits will automatically have higher scores. Train a regression model to predict score, a good proxy for popularity, using data from political subreddits. Evaluate the model’s performance based on \\(RMSE\\) and \\(R^2\\), and analyze feature importance (if possible) to understand which factors most influence post popularity.\n\n\nIdea 10: Predicting Political Leaning of Comments and Posts\nBusiness goal: Automatically classify comments and posts as left-leaning, right-leaning, or neutral to understand the distribution of political perspectives within and across subreddits.\nTechnical proposal: Utilize as pre-trained model to predict the political leanings of comments and posts. Identify a pre-trained model that is effective at classifying political text based on its political leaning. Apply the model to comments and posts in political subreddits, then analyze the distribution of political leanings across subreddits. Compare the distribution of political leanings in different subreddits with the stated political affiliation of the subreddit to assess alignment.\n\n\nIdea 11: Predicting Number of Comments\nBusiness goal: Develop a model to predict the if a post will have a small amount, a moderate amount or many comments.\nTechnical proposal: Create a categorical variable for the number of comments linked to a submission based on percentile to classify if the post has a large, moderate, or small number of comments. Train a machine learning model to predict the number of comments, apply the model to the submissions in the dataset, and analyze the results. Calculate the most important features in predicting the comment number. Compare this across subreddits and other features.",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Through the exploratory data analysis (EDA) process, we aimed to understand the political engagement on Reddit by analyzing the comments and submissions from six major political subreddits: r/Conservative, r/democrats, r/Liberal, r/Libertarian, r/politics, and r/Republican. We focused on understanding the frequency of conversations about Trump and Biden, the controversiality of threads across the subreddits, and the distribution of political contributions by top authors.\nOur analysis revealed that political subreddits tend to discuss Trump more frequently than Biden, though this trend changes over time based on the subreddit and current events. We found that discussions on r/Republican then to be the most contraversial, while r/democrats tend to have the least controversial threads. There is not a clear link between the number of comments on a thread, the score of the thread, and the controversiality of the comments; however, this could be influenced by the fact that score is the sum of upvotes and downvotes and obfuscates the true popularity of a thread. Additionally, we found that the top posters in each subreddit tend to make up a large percentage of the total posts.\nOur analysis indicates that the political climate on Reddit is complex and multifaceted, with different subreddits having different levels of engagement and controversiality. Based on this research, we continue our analysis using natural language processing (NLP) and machine learning (ML) techniques to further understand the political climate on Reddit.",
    "crumbs": [
      "Home",
      "EDA"
    ]
  },
  {
    "objectID": "eda.html#sec-subreddit-engagement",
    "href": "eda.html#sec-subreddit-engagement",
    "title": "Exploratory Data Analysis",
    "section": "Subreddit Political Engagement",
    "text": "Subreddit Political Engagement\nOur first goal when performing exploratory data analysis was to identify the subreddits that have a large number of political posts that mention Biden or Trump. To do this, we examined the comments from many different politically oriented subreddits. We found that the subreddit r/Politics had by far the most mentions of the two presidents. However, given that the main purpose of this project is to compare and contrast the political engagement and climate of the major political parties, we decided to focus on six major subreddit, as seen in Table 1. These subreddits are r/Liberal, r/Conservative, r/Republican, r/Libertarian, r/democrats, and r/politics. We filtered the submissions to select only those submissions that mentioned Biden or Trump in order to focus on broad political trends. To analyze the submissions, we looked at the average score and the average number of comments per subreddit.\n\n\n\n\nTable 1: Median scores reveal that while all subreddits have some popular posts, most submissions have low scores and few comments.\n\n\n\n\n\n\n\n\n\n\nPolitical Subreddit Submissions Mentioning Biden or Trump\n\n\nSummary statistics on the number of comments and score by subreddit per submission\n\n\n\n\nNumber of Comments\nScore\nNumber of Submissions\n\n\nAverage\nMedian\nAverage\nMedian\n\n\n\n\nr/Conservative\n\n17.5\n0\n54.2\n1\n27,756\n\n\nr/democrats\n\n14.4\n1\n94.5\n1\n8,582\n\n\nr/Liberal\n\n18.8\n0\n50.2\n1\n1,078\n\n\nr/Libertarian\n\n17.0\n0\n32.9\n1\n561\n\n\nr/politics\n\n85.1\n2\n724.9\n1\n57,617\n\n\nr/Republican\n\n5.9\n1\n23.8\n5\n6,762\n\n\n\nSource: Reddit data from June 2023 to July 2024\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1 shows the average number of comments received by each post in the different subreddits. The number of comments that the average post receives can give us a comparative measure of overall engagement across the subreddits when it comes to the presidential election. We can see from the plot that r/politics has the highest number of comments per submission, and r/Republican has the lowest, with less than 7. r/democrats has just under 15 comments per submission, while r/Conservative and r/Libertarian have slightly ore at ~17-18 comments per submission.\n\n\n\n\n\n\nFigure 1: r/politics has the highest average number of comments per submission since it is the largest subreddit in our analysis.\n\n\n\nNext, we looked at the frequency of mentions of Biden and Trump between the subreddits. Figure 2 shows whether the users in each subreddit are more concerned with their parties candidate or the opposing parties candidate. This plot also gives us an idea of the number of total posts mentioning the candidates in each subreddit. We can see that the largest subreddit in terms of number of candidate mentioning posts was by far r/Conservative, which had well over double the posts of the next subreddit. r/democrats and r/Republicans had a similar number of candidate mentioning posts, while r/Libertarian expectedly had a small fraction of the number of posts achieved by the other subreddits. In terms of specific candidate mentions, only r/democrats had a significant difference, with Trump being mentioned more often than Biden. Both r/Libertarian and r/Conservative had slightly more trump mentions, while r/Republican mentioned Biden slighlty more.\n\n\n\n\n\n\nFigure 2: Most political subreddits mention Trump slightly more than Biden.\n\n\n\nThe next aspect of the data we wanted to look into was the average score per subreddit per candidate over time. The score of a reddit submission is the total number of thumbs up votes it receives minus the total number of thumbs down votes it receives. Figure 3 shows plots for each subreddit, with the red line indicating mentions of Trump and the blue line indicating mentions of Biden. These are the weekly averages. We can see that across all subreddits there is an issue with the data around September 2023, when the scores are all 0. Across most of the subreddits neither of the candidates has a decisively higher score, with many subreddits seeing a large variation in the weekly averages of both candidates. The one exception to this is r/politics, where the average score is consistently higher for posts mentioning Trump, an interesting trend for a non-partisan subreddit.\n\n\n\n\n\n\nFigure 3: Posts on r/politics tend to have higher scores if they mention Trump.\n\n\n\n\n\n\n\n\n\nThe code used for this section is available here.",
    "crumbs": [
      "Home",
      "EDA"
    ]
  },
  {
    "objectID": "eda.html#sec-controversiality",
    "href": "eda.html#sec-controversiality",
    "title": "Exploratory Data Analysis",
    "section": "Controversiality vs. Comment Count",
    "text": "Controversiality vs. Comment Count\nOur second EDA goal was to understand the relationship between controversiality and comment count in different posts. First, we wanted to understand if certain subreddits were prone to more controversial posts than others. We started by examining the comments dataset, as summarized in Table 2.\n\n\n\n\nTable 2: There is not a clear pattern between the average controversiality, comment count, and the submission score, with the extreme values falling in across different subreddits.\n\n\n\n\n\n\n  \n    \n      Political Subreddit Comments Mentioning Biden or Trump\n    \n    \n      Summary statistics on the score, controversiality, and length of comments by subreddit\n    \n    \n      \n      Average Score\n      Average Controversiality\n      \n        Comment Length (Words)\n      \n    \n    \n      Average\n      Median\n    \n  \n  \n    r/Conservative\n7.50\n0.05\n14.44\n1\n    r/democrats\n6.79\n0.02\n24.21\n13\n    r/Liberal\n4.49\n0.04\n26.75\n13\n    r/Libertarian\n5.98\n0.07\n33.96\n19\n    r/politics\n11.52\n0.03\n29.56\n17\n    r/Republican\n3.31\n0.10\n32.97\n17\n  \n  \n    \n      Source: Reddit data from June 2023 to July 2024\n    \n  \n  \n    \n       Pink indicates lower values, while blue indicates higher values.\n    \n  \n\n\n\n\n\n\n\nFrom the Table 2, we see that r/Republican has the highest average controversiality, followed by r/Libertarian. r/democrats has the lowest average controversiality. We also see that r/Republican has the lowest average score, while r/politics has the highest. r/Libertarian seems to have the longest comments on average, with both the highest mean and median word count per comment.\nAs seen in Figure 4 found that r/Republican had the most percentage of posts being considered controversial, with almost 8%. This was followed by r/Conservative and r/Libertarian. Interestingly, there seems to be a clear political divide between the rate of controversial posts across the aisle, with more conservative subreddits having a higher rate of controversial posts.\n\n\n\n\n\n\nFigure 4: Left leaning subreddits tend to have fewer controversial comments than right leaning subreddits.\n\n\n\nUsing the link between submissions and comment count, we can see the relationship between the number of comments on a submission, the size of the thread, and the average controversiality of the comments. A comment is rated as either being controversial or not, so a higher average controversiality indicates that more comments on a submission are controversial. Generally, controversiality seems to go down as the number of comments goes up, as seen in Figure 5.\n\n\n\n\n\n\nFigure 5: As the size of the thread goes up, the average controversiality goes down.\n\n\n\nTo investigate further, we divide the plot by subreddit in Figure 6. We gain more information about each subreddit’s relationship between controversiality and comment count. We see that r/Conservative and r/politics have more controversial posts, but overall more comments leads to less controversiality. r/Republicans shows that more controversiality leads to more comments, as does r/Libertarian. r/democrats and r/Liberals generally have few controversial posts, and more comments doesn’t necessarily stem from more controversiality.\n\n\n\n\n\n\nFigure 6: Left leaning subreddits tend to have low controversiality for all sized threads, while right leaning subreddits tend towards more controversiality.\n\n\n\nWhen looking at the top 5 submissions with the most comments per subreddit, we see that the most controversial posts are not necessarily the ones with the most comments. In Table 3, the most controversial posts do not have the most comments. There is also not a clear pattern between the average controversiality, comment count, and the submission score.\n\n\n\n\nTable 3: For each subreddit, the threads with the most comments have a wide range of scores and controversiality levels.\n\n\n\n\n\n\n\n\n\n\nTop 5 Threads by Subreddit\n\n\nTop 5 threads with the most comments for each political subreddit on posts mentioning Biden or Trump\n\n\n\n\nNumber of Comments\nAverage Comment Controversiality\nScore\n\n\n\n\nr/Conservative\n\n4,984\n0.06\n12,202\n\n\n\n4,734\n0.01\n501\n\n\n\n3,773\n0.03\n6,291\n\n\n\n3,620\n0.03\n0\n\n\n\n3,453\n0.09\n4,319\n\n\nr/democrats\n\n4,574\n0.02\n5,942\n\n\n\n1,444\n0.03\n1,214\n\n\n\n1,324\n0.05\n1,806\n\n\n\n853\n0.05\n632\n\n\n\n829\n0.01\n3,086\n\n\nr/Liberal\n\n330\n0.06\n83\n\n\n\n314\n0.07\n981\n\n\n\n313\n0.03\n323\n\n\n\n310\n0.06\n330\n\n\n\n284\n0.06\n155\n\n\nr/Libertarian\n\n622\n0.02\n924\n\n\n\n526\n0.16\n115\n\n\n\n525\n0.23\n544\n\n\n\n473\n0.11\n1,280\n\n\n\n387\n0.00\n1\n\n\nr/politics\n\n54,712\n0.04\n12,296\n\n\n\n43,122\n0.01\n89,077\n\n\n\n34,859\n0.06\n33,027\n\n\n\n28,504\n0.05\n55,882\n\n\n\n25,506\n0.03\n31,753\n\n\nr/Republican\n\n539\n0.10\n290\n\n\n\n399\n0.00\n497\n\n\n\n294\n0.27\n248\n\n\n\n217\n0.02\n285\n\n\n\n217\n0.02\n629\n\n\n\nSource: Reddit data from June 2023 to July 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWhen we compare average controversiality with submission score, we see that some subreddits tend towards higher scores with less controversiality, while other tend towards lower scores with more controversiality. In Figure 7, we see that r/democrats tends to have high scores with less controversiality, while r/Republicans tends to have lower scores with more controversiality. r/Conservative and r/Libertarian tend to have higher scores with more controversiality. Threads on r/politics tend to have small scores relative to the size of the subreddit, but the controversiality is relatively low.\n\n\n\n\n\n\nFigure 7: Left leaning subreddits tend to have non-controversial threads, even as the score increases. Right leaning subreddits increase in controversiality as the score increases.\n\n\n\n\n\n\n\n\n\nThe code used for this section is available here and here.",
    "crumbs": [
      "Home",
      "EDA"
    ]
  },
  {
    "objectID": "eda.html#sec-top-authors",
    "href": "eda.html#sec-top-authors",
    "title": "Exploratory Data Analysis",
    "section": "Distribution of Political Contributions by Top Authors",
    "text": "Distribution of Political Contributions by Top Authors\nOur third EDA goal was to look at the distribution of the candidate mentioning posts by the top authors. In Figure 8, we plotted the number of posts from the top ten authors for each subreddit as a percentage of the total candidate mentioning posts found in that subreddit. One of the most interesting finds in Figure 8 is that r/Republican has two authors that make up over 50% of all posts in the subreddit. This is followed by the third highest posting author making up less than 4% of the remianing posts, and eaech following author making up a slightly smaller percent. r/democrats shares a similar overall distribution, with one author comprising around 7% of the posts followed by a steep drop to the number two author who made up roughly 2% of the posts. The same steady decrease can be observed in the subsequent authors. r/Conservative, r/Liberal, and r/Libertarian share similarly skewed distributions, with a rapid decrease in percentage from the top poster to each subsequent poster before evening out at around 1% by the tenth author. r/politics bucks the trend slightly as the highest posters make up a smaller percentage of the total posts, with the top poster making up around 1% of the total posts. This is not suprising given that r/politics is by far the largest subreddit in our analysis.\n\n\n\n\n\n\nFigure 8: Each subreddit has regular posters that make up a large percentage of the total posts mentioning Biden or Trump.\n\n\n\n\n\n\n\n\n\nThe code used for this section is available here.",
    "crumbs": [
      "Home",
      "EDA"
    ]
  },
  {
    "objectID": "nlp.html",
    "href": "nlp.html",
    "title": "Natural Language Processing",
    "section": "",
    "text": "Executive summary\nSentiment analysis plays a critical role in understanding Reddit’s political discussion climate, revealing the emotional tone of conversations about Presidential candidates Donald Trump and Joe Biden. Across subreddits, most submissions exhibited negative sentiment, but the percentage of positive sentiment fluctuated over time and varied by subreddit. Submissions about Trump showed higher positive sentiment in subreddits aligned with his political party, and the reverse was true for Biden. By comparing sentiment trends with Presidential Job Approval Rates, we gained a nuanced understanding of these dynamics, noting that approval rates often aligned between the positive sentiment percentages of politically opposing subreddits.\nWe then analyzed text data to uncover patterns in language reflecting key themes in political discourse. Using CountVectorizer and Latent Dirichlet Allocation (LDA), we identified trends in word usage that formed distinct topics across subreddits. Cross-referencing these dominant terms with Google Trends confirmed their alignment with broader public interest and search behaviors. Further, calculating Term Frequency-Inverse Document Frequency (TF-IDF) scores highlighted the importance of specific words within individual subreddits and their influence on submission sentiment.\nFinally, we synthesized these findings to assess the distinctiveness of each subreddit. By examining correlations between sentiment, scores, and post popularity, we found that r/Libertarian was the most distinct, while r/politics appeared the most moderate among the six subreddits analyzed.\n\n\nAnalysis report\nTo analyze Reddit sentiment, we concatenated submission titles and bodies, removed special characters and numbers, and applied PySpark NLP models.\n\nComparison Between Reddit Sentiment and Presidential Job Approval Rates\nOur analysis began by looking at the sentiment distribution across subreddits. As illustrated in Figure 1, nearly 60% of Biden-related submissions had negative sentiment across all subreddits. Trump-related submissions displayed similar trends, except in r/Libertarian, where positive sentiments outweighed negatives, with fewer than 40% negative sentiments.\n\n\n\n\n\n\nFigure 1: Sentiment Distribution by two candidates\n\n\n\nThen we examined how sentiment trends evolved over time. As shown in Figure 2, the percentage of positive sentiments fluctuates significantly across subreddits. For example, r/politics consistently exhibits low positive sentiment, while r/Libertarian shows the most variability. These differences may relate to subreddit membership sizes, withr/politics having a much larger member base than r/Libertarian.\n\n\n\n\n\n\nFigure 2: Postive Sentiments by subreddits over time\n\n\n\nWe then filtered submissions to focus on posts about each presidential candidate during specific periods. In Trump-related posts, r/Republican and r/Conservative generally showed higher positive sentiment compared to subreddits aligned with opposing views. Conversely, Biden-related posts exhibited higher positive sentiment in r/Democrats and r/Liberal. Notably, r/politics maintained lower positive sentiment for both candidates, while r/Libertarian showed pronounced peaks favoring Trump over Biden, as seen in Figure 3 and Figure 4.\n\n\n\n\n\n\nFigure 3: Postive Sentiments on Trump related posts\n\n\n\n\n\n\n\n\n\nFigure 4: Postive Sentiments on Biden related posts\n\n\n\n\n\n\n\n\n\nThe code used for this section is available here.\n\n\n\n\n\nLexical Trends Analysis of Key Terms in Trump and Biden Related Reddit Submissions\nIn addition to sentiment analysis, we employed three NLP/ML models—CountVectorizer, LDA, and TF-IDF—to explore lexical patterns in Reddit submissions. We processed the text by tokenizing, normalizing, and removing stopwords, ensuring clean data for word frequency analysis and topic modeling.\n\nCountVectorizer\nAs seen in Figure 5, generic terms like case and court initially dominated word frequency counts across all subreddits. After filtering out these terms, subreddit-specific word trends emerged.\n\n\n\n\n\n\nFigure 5: Most common words in all 6 subreddits\n\n\n\nFor instance, in Table 1, terms such as hunter and border were prominent in r/Conservative and r/Republican. In contrast, harris was most frequent in r/Democrats, reflecting Kamala Harris’s role as Vice President. Meanwhile, israel and war stood out in r/Libertarian, highlighting its unique discourse. Common terms like news and administration still appeared across most subreddits.\n\n\n\n\n\n\nr/Conservative\n\n\nr/ Republican\n\n\nr/democrats\n\n\nr/Liberal\n\n\nr/Libertarian\n\n\nr/politics\n\n\n\n\n\n\n0\n\n\nhunter (6.08%)\n\n\nhunter (8.07%)\n\n\nharris (3.87%)\n\n\nnews (4.73%)\n\n\nconvention (6.6%)\n\n\nharris (2.88%)\n\n\n\n\n1\n\n\nborder (2.67%)\n\n\nborder (3.9%)\n\n\nvoters (3.71%)\n\n\nvoters (4.64%)\n\n\nisrael (6.24%)\n\n\npoll (2.6%)\n\n\n\n\n2\n\n\nwhite (2.31%)\n\n\nreport (2.51%)\n\n\nadministration (3.34%)\n\n\nright (4.17%)\n\n\nwar (4.81%)\n\n\ncnn (2.58%)\n\n\n\n\n3\n\n\npoll (2.14%)\n\n\nfbi (2.13%)\n\n\nnews (3.03%)\n\n\nwin (4.08%)\n\n\nright (4.28%)\n\n\ndocuments (2.47%)\n\n\n\n\n4\n\n\nadministration (2.05%)\n\n\nwhite (2.07%)\n\n\nstate (2.92%)\n\n\ndemocracy (3.8%)\n\n\nadministration (4.1%)\n\n\ngeorgia (2.47%)\n\n\n\n\n5\n\n\nmedia (2.02%)\n\n\nnews (2.06%)\n\n\nmillion (2.44%)\n\n\nvoting (3.53%)\n\n\nwant (3.92%)\n\n\nmoney (2.41%)\n\n\n\n\n6\n\n\nnews (2.0%)\n\n\ndoj (2.0%)\n\n\nwhite (2.14%)\n\n\nsupport (3.53%)\n\n\nhunter (3.92%)\n\n\nvoters (2.41%)\n\n\n\n\n7\n\n\nvoters (1.9%)\n\n\nillegal (1.94%)\n\n\nfederal (2.11%)\n\n\nwins (3.43%)\n\n\nsupport (3.74%)\n\n\npresidential (2.41%)\n\n\n\n\n8\n\n\nstate (1.89%)\n\n\npoll (1.82%)\n\n\nstates (2.07%)\n\n\nwant (3.43%)\n\n\ncrimes (3.57%)\n\n\nnews (2.4%)\n\n\n\n\n9\n\n\nreport (1.85%)\n\n\nrally (1.73%)\n\n\nsupport (2.04%)\n\n\nlegal (3.34%)\n\n\nbillion (3.21%)\n\n\nclassified (2.29%)\n\n\n\n\n\nTable 1: Top 10 Word Counts in 6 subreddits\n\n\n\nLatent Dirichlet Allocation (LDA)\nIn this part, we used LDA for our topic modeling to explore the underlying themes in the data and understand the distribution of terms across topics. We uncovered themes aligned with CountVectorizer findings. Key terms like hunter, border, and harris were grouped into distinct topics. The term israel, although most prevalent in r/Libertarian, also appeared in a topic alongside war and ukraine, suggesting a thematic focus on international issues.\n\n\n\n\n\n\nTopic 1\n\n\nTopic 2\n\n\nTopic 3\n\n\nTopic 4\n\n\nTopic 5\n\n\n\n\n\n\n0\n\n\njudge\n\n\ncoming\n\n\nisrael\n\n\nborder\n\n\ncourt\n\n\n\n\n1\n\n\ncase\n\n\nlies\n\n\nrally\n\n\nadministration\n\n\nvoters\n\n\n\n\n2\n\n\nhunter\n\n\nwarns\n\n\nmoney\n\n\ntexas\n\n\ndemocrats\n\n\n\n\n3\n\n\ntrial\n\n\nhouse\n\n\ntrial\n\n\nwall\n\n\ncampaign\n\n\n\n\n4\n\n\ndocuments\n\n\nnews\n\n\nwar\n\n\ncrisis\n\n\nhouse\n\n\n\n\n5\n\n\nclassified\n\n\nepstein\n\n\ncampaign\n\n\nstudent\n\n\ndebate\n\n\n\n\n6\n\n\nspecial\n\n\nheard\n\n\nukraine\n\n\ngaza\n\n\npoll\n\n\n\n\n7\n\n\ncourt\n\n\ncountry\n\n\nguilty\n\n\npolicy\n\n\nsupreme\n\n\n\n\n8\n\n\norder\n\n\nwhite\n\n\nhush\n\n\nplan\n\n\nharris\n\n\n\n\n9\n\n\nfbi\n\n\nhunter\n\n\nterm\n\n\nnetanyahu\n\n\npresidential\n\n\n\n\n\nTable 2: Top 10 Words in 5 Topics\n\n\n\nGoogle Trends\nSince we found out dominant words formed a political discussion, we compared them with related topics and queries on Google Trends for Trump and Biden. Terms like hunter and israel appeared as related queries for Biden, while no other overlap was observed.\n\n\n\n\n\n\nTrump\n\n\nBiden\n\n\n\n\n\n\n0\n\n\ndebate (22)\n\n\nhunter (25)\n\n\n\n\n1\n\n\nunited (17)\n\n\nunited (18)\n\n\n\n\n2\n\n\ncourt (17)\n\n\nisrael (13)\n\n\n\n\n3\n\n\nmedia (15)\n\n\ndebate (13)\n\n\n\n\n4\n\n\nshot (14)\n\n\nimpeachment (12)\n\n\n\n\n5\n\n\ntrial (14)\n\n\nspeech (10)\n\n\n\n\n6\n\n\nindictment (12)\n\n\njill (10)\n\n\n\n\n7\n\n\nspeaker (12)\n\n\nhouse (9)\n\n\n\n\n8\n\n\nmug (11)\n\n\nconference (9)\n\n\n\n\n9\n\n\nhouse (11)\n\n\nunion (9)\n\n\n\n\n\nTable 3: Google Trends related-search terms on Trump and Biden\n\n\n\n\n\n\n\nThe code used for this section is available here.\n\n\n\n\n\n\nImpact of Dominant Terms on Shaping Political Discussions on Reddit\nFiguring out some terms dominating the political discussion on those subreddits, we focuse on how important they are shaping the political discussion. We used TF-IDF to calculate the importance score of those terms.\nScores were highly skewed across subreddits. Most subreddits had an average score around 4, but in r/Liberal, scores were more evenly distributed in positive sentiment submissions. Conversely, in r/Libertarian, submissions with negative sentiment showed a smoother score distribution due to potential fewer positive submissions.\n\n\n\n\n\n\nFigure 6: TF-IDF Score on term administration\n\n\n\nNext, we checked out the term border and israel. As Table 4 and Table 5 show, these two words are very distinct on specific subreddit and sentiment. The term border had a higher average importance score in r/Democrats for negative sentiment submissions. Similarly, the term israel had a higher average score in r/politics for neutral sentiment submissions.\n\n\n\n\n\n\nsubreddit\n\n\nTF_IDF Score on ‘Border’\n\n\n\n\nsentiment\n\n\n\n\nnegative\n\n\nneutral\n\n\npositive\n\n\n\n\n\n\n0\n\n\nConservative\n\n\n4.400743\n\n\n4.282261\n\n\n4.429995\n\n\n\n\n1\n\n\nLiberal\n\n\n4.096076\n\n\n0\n\n\n4.096076\n\n\n\n\n2\n\n\nLibertarian\n\n\n4.096076\n\n\n0\n\n\n0\n\n\n\n\n3\n\n\nRepublican\n\n\n4.804039\n\n\n4.973807\n\n\n4.232611\n\n\n\n\n4\n\n\ndemocrats\n\n\n6.085598\n\n\n4.096076\n\n\n4.096076\n\n\n\n\n5\n\n\npolitics\n\n\n4.242364\n\n\n4.247782\n\n\n4.159093\n\n\n\n\n\nTable 4: TF_IDF Score on term ‘border’ by Sentiments and Subreddits\n\n\n\n\n\n\n\nsubreddit\n\n\nTF_IDF Score on “Israel”\n\n\n\n\nsentiment\n\n\n\n\nnegative\n\n\nneutral\n\n\npositive\n\n\n\n\n\n\n0\n\n\nConservative\n\n\n4.336109\n\n\n0\n\n\n4.173768\n\n\n\n\n1\n\n\nLiberal\n\n\n5.266312\n\n\n0\n\n\n4.069423\n\n\n\n\n2\n\n\nLibertarian\n\n\n5.634586\n\n\n0\n\n\n0\n\n\n\n\n3\n\n\nRepublican\n\n\n4.461657\n\n\n0\n\n\n4.069423\n\n\n\n\n4\n\n\ndemocrats\n\n\n4.853890\n\n\n4.069423\n\n\n4.069423\n\n\n\n\n5\n\n\npolitics\n\n\n4.385980\n\n\n8.138845\n\n\n4.069424\n\n\n\n\n\nTable 5: TF_IDF Score on term ‘israel’ by Sentiments and Subreddits\n\n\n\n\n\n\n\nThe code used for this section is available here.\n\n\n\n\n\nIdentifying the Most Extreme Subreddits in Political Discourse\nSeeing the general negative sentiments trend, lexical trends, and importance scores on selected terms, we decided to see how extreme discussions these subreddits have. As we showed in Figure 1, Figure 3, and Figure 4, r/Libertarian has the most shifting sentiments over time with higher positive sentiments on Trump related submissions than on Biden related submissions. This may shaped their discussion topics as israel and war appeared in top 10 most frequent words in Table 1. On other terms, the subreddit has only negative sentiments on term border and israel showing their political discussion was very controversial.\nNext thing we considered was how sentiments impact on submissions’ number of comments and popularity score. We found out that neutral sentiment submissions about Biden in r/Democrats garnered the highest comment counts. Conversely, Trump-related posts with negative sentiment attracted more comments in r/Conservative, r/Liberal, and r/Libertarian. Submissions in r/Republican had fewer comments overall. On thing to note here is that there was no comments on neutral sentiment submissions about Trump in r/Libertarian. This showed us that the subreddits may be more biploar than the other subreddits.\n\n\n\n\n\n\nFigure 7: Average Number of Comments by sentiments and subreddits\n\n\n\nWhen it comes to the score, submissions in r/Democrats consistently had the highest scores across all sentiment categories. Meanwhile, r/Libertarian showed again extreme variability, with Trump-related neutral sentiment submissions averaging a score of 1, compared to 89 for neutral Biden-related submissions.\n\n\n\n\n\n\nFigure 8: Average Score by sentiments and subreddits\n\n\n\n\n\n\n\n\n\nThe code used for this section is available here.\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "NLP"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Conclusion",
    "section": "",
    "text": "Throughout our research on political discussions occuring on Reddit, we uncovered the divide between right leaning and left leaning conversations. While politics of all kinds are discussed on the forum, we identified trends that highlight the unique ways that users communicate within distinctive subcommunities.\nIn our initial exploration of the data, we found that political conversations across all ends of the political spectrum tend to focus more on Trump than Biden. While this can change over time within subreddits, this trend is especially apparent on general political subreddits like r/politics. We also found that left leaning subreddits strayed away from controversial topics, while right leaning subreddits were much more likely to have controversial threads as demonstrated in Figure 1. In our initial exploration of the data, we found that political conversations across all ends of the political spectrum tend to focus more on Trump than Biden. While this can change over time within subreddits, this trend is especially apparent on general political subreddits like r/politics. We also found that left leaning subreddits strayed away from controversial topics, while right leaning subreddits were much more likely to have controversial threads as demonstrated in Figure 1. This analysis of threads also showed that generally, the more comments a thread has, the less controversial it is, indicating that users may be more likely to engage in discussions that are less controversial. Analyses also showed how a few users dominate conversations in all subreddits, with a small number of users contributing a large percent of posts to each subreddit.\n\n\n\n\n\n\nFigure 1: Left leaning subreddits tend to have low controversiality for all sized threads, while right leaning subreddits tend towards more controversiality.\n\n\n\nOur analysis of Reddit political discussion revealed distinct sentiment and lexical trends across six subreddits. Sentiment analysis showed r/politics consistently had low positive sentiment, while r/Libertarian exhibited the most variability, often favoring Trump. Lexical analysis using CountVectorizer, LDA, and TF-IDF highlighted key terms like hunter, border, and israel, which varied in prominence by subreddit and sentiment. Google Trends partially validated these findings, with some overlap in related queries. Engagement metrics further illustrated extremes, with r/Democrats achieving the highest submission scores and comment counts, while r/Libertarian displayed stark variability. These patterns reflect subreddit-specific biases and their impact on political discourse.\nOur machine learning analysis of subreddit submission data yielded mixed results. Across our experiments we found tree based models to give the best results. We were able to predict the subreddit of a post 56% of the time, versus a most common feature accuracy 17%. We got better results with predicting the score of a subreddit, where we were able to achieve a 74% accuracy with a decision tree model. Score can be used as a proxy for popularity. We found that the self selection of subreddit was a poor proxy for political leaning. Furthermore, this proxy was hard to predict with our data. For political leaning prediction, 2e achieved an accuracy of 83%, which was only 6% over the most common feature accuracy of 77%. The last ML experiment was to predict the number of comments. Here we were able to achieve an accuracy of 76%. The number of comments can be used as a proxy for the controversiality of a post, as people will comment on and answer comments more frequently on controversial posts. In Figure 2, we see that all four predictive modeling experiments have a ROC curve that is above the baseline, and it’s clear that our popularity prediction model was the most successful. All models give us insight into the separation of the data, allowing us to understand the differences in text between the subreddits.\n\n\n\n\n\n\nFigure 2: ROC Curves for Predictive Models",
    "crumbs": [
      "Home",
      "Conclusion"
    ]
  },
  {
    "objectID": "summary.html#summary",
    "href": "summary.html#summary",
    "title": "Conclusion",
    "section": "",
    "text": "Throughout our research on political discussions occuring on Reddit, we uncovered the divide between right leaning and left leaning conversations. While politics of all kinds are discussed on the forum, we identified trends that highlight the unique ways that users communicate within distinctive subcommunities.\nIn our initial exploration of the data, we found that political conversations across all ends of the political spectrum tend to focus more on Trump than Biden. While this can change over time within subreddits, this trend is especially apparent on general political subreddits like r/politics. We also found that left leaning subreddits strayed away from controversial topics, while right leaning subreddits were much more likely to have controversial threads as demonstrated in Figure 1. In our initial exploration of the data, we found that political conversations across all ends of the political spectrum tend to focus more on Trump than Biden. While this can change over time within subreddits, this trend is especially apparent on general political subreddits like r/politics. We also found that left leaning subreddits strayed away from controversial topics, while right leaning subreddits were much more likely to have controversial threads as demonstrated in Figure 1. This analysis of threads also showed that generally, the more comments a thread has, the less controversial it is, indicating that users may be more likely to engage in discussions that are less controversial. Analyses also showed how a few users dominate conversations in all subreddits, with a small number of users contributing a large percent of posts to each subreddit.\n\n\n\n\n\n\nFigure 1: Left leaning subreddits tend to have low controversiality for all sized threads, while right leaning subreddits tend towards more controversiality.\n\n\n\nOur analysis of Reddit political discussion revealed distinct sentiment and lexical trends across six subreddits. Sentiment analysis showed r/politics consistently had low positive sentiment, while r/Libertarian exhibited the most variability, often favoring Trump. Lexical analysis using CountVectorizer, LDA, and TF-IDF highlighted key terms like hunter, border, and israel, which varied in prominence by subreddit and sentiment. Google Trends partially validated these findings, with some overlap in related queries. Engagement metrics further illustrated extremes, with r/Democrats achieving the highest submission scores and comment counts, while r/Libertarian displayed stark variability. These patterns reflect subreddit-specific biases and their impact on political discourse.\nOur machine learning analysis of subreddit submission data yielded mixed results. Across our experiments we found tree based models to give the best results. We were able to predict the subreddit of a post 56% of the time, versus a most common feature accuracy 17%. We got better results with predicting the score of a subreddit, where we were able to achieve a 74% accuracy with a decision tree model. Score can be used as a proxy for popularity. We found that the self selection of subreddit was a poor proxy for political leaning. Furthermore, this proxy was hard to predict with our data. For political leaning prediction, 2e achieved an accuracy of 83%, which was only 6% over the most common feature accuracy of 77%. The last ML experiment was to predict the number of comments. Here we were able to achieve an accuracy of 76%. The number of comments can be used as a proxy for the controversiality of a post, as people will comment on and answer comments more frequently on controversial posts. In Figure 2, we see that all four predictive modeling experiments have a ROC curve that is above the baseline, and it’s clear that our popularity prediction model was the most successful. All models give us insight into the separation of the data, allowing us to understand the differences in text between the subreddits.\n\n\n\n\n\n\nFigure 2: ROC Curves for Predictive Models",
    "crumbs": [
      "Home",
      "Conclusion"
    ]
  },
  {
    "objectID": "summary.html#future-work",
    "href": "summary.html#future-work",
    "title": "Conclusion",
    "section": "Future Work",
    "text": "Future Work\nOur project analyzed a one-year period from June 2023 to July 2024, capturing trends in political subreddits during this time. Exploring earlier or more recent data could reveal how subreddit dynamics evolve with political events.\nThere are many possibilities for filtering the data, such as by subreddits, sentiments, presidential candidates, or dummy variables based on specific terms. However, these options present challenges when trying to create reasonable and interpretable visualizations, as the complexity of the filters can obscure the clarity of the figures.\nOur analysis was limited to political subreddits, and we were unable to expand into non-political subreddits. Exploring discussions in broader contexts could provide a more comprehensive understanding of how political discourse permeates other online communities.\nWhile we focused heavily on submissions, future work should examine submission-comment interactions to better understand how discussions develop and how comments shape or respond to initial posts. This would provide deeper insights into the conversational dynamics of political subreddits.",
    "crumbs": [
      "Home",
      "Conclusion"
    ]
  },
  {
    "objectID": "author.html",
    "href": "author.html",
    "title": "Patterns in U.S. Political Discussions",
    "section": "",
    "text": "This website was created and designed by Brian Kwon, Aaron Schwall, and Marion Bauman as part of DSAN 6000: Big Data, a graduate level data science course at Georgetown University.\n\nMarion Bauman\n\n\n\n\n\n\nMarion Bauman is graduating from the Master of Science in Data Science program at Georgetown University in December 2024. She has worked as a data aanalyst at a consulting firm, supporting the Department of Veterans Affairs with data analysis and visualization. She currently works as a teaching assistant for the Advanced Natural Language Processing course at Georgetown, and she has an interest in natural language processing and machine learning. Check out her work on GitHub or connect with her on LinkedIn.\n\n\n\n\nBrian Kwon\n\n\n\n\n\n\nBrian Kwon is a graduate student at Georgetown University studying data science. He is currently working as a teaching assistant for the deep learning course. Find him on LinkedIn and GitHub.\n\n\n\n\nAaron Schwall\n\n\n\n\n\n\nAaron Schwall is studying data science at Georgetown University. He currently works as a software engineer at L3Harris Technologies. Find him on LinkedIn and GitHub.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Reference",
    "section": "",
    "text": "1. Presidential Job Approval Rate\nhttps://www.presidency.ucsb.edu/statistics/data/joseph-r-biden-public-approval\n2. SerpAPI (Google Trends)\nhttps://serpapi.com/google-trends-api\n\n\n\n\n\n\nThe code used for SerpAPI is available here.\n\n\n\n\n\n\n Back to top"
  }
]